# ğŸ§  DeepTalk DeepSeek-R1 GUI

A Streamlit-based Proof-of-Concept (PoC) for inferencing **DeepSeek-R1** and **Qwen-QwQ** models via the OpenAI API.  
This GUI enables **Chain-of-Thought (CoT) handling**, ensuring proper contextual reasoning for LLM interactions while preventing stale CoT data from polluting session memory.

---

## ğŸ“– Introduction

DeepSeek-R1 and Qwen-QwQ are "reasoning" models that begins with **Chain-of-Thought (CoT) reasoning**, encapsulated within `<think></think>` tags. This mechanism allows the model to break down complex problems and generate structured responses.

### **The Problem: Uncontrolled CoT Context Growth**
Most LLM clients currently **retain and resend CoT outputs** in session history, leading to:

1. **Context Window Bloat** â€“ Unnecessary CoT data rapidly consumes available tokens.
2. **Irrelevant CoT Carryover** â€“ Old reasoning outputs persist and interfere with new queries.
3. **Redundant Processing** â€“ The model repeatedly processes stale CoT, degrading response quality.

### **The Solution: Controlled CoT Retention**
This GUI **isolates CoT reasoning during generation**, ensuring:

âœ” CoT reasoning is **used for inference but not stored** in session memory.  
âœ” CoT explanations remain **visible for review** but are excluded from future prompts.  
âœ” The conversation buffer is **optimized** for better model efficiency and coherence.

---

## ğŸš€ Features

- **ğŸ“¡ Streamlit UI** â€“ Intuitive interface for interacting with DeepSeek-R1 and Qwen-QwQ.
- **ğŸ§  CoT Segmentation** â€“ Extracts `<think></think>` reasoning while **preventing stale CoT retention**.
- **ğŸ“¡ Real-time Streaming** â€“ Displays LLM responses **as they are generated**.
- **ğŸ“Œ Live CoT View** â€“ CoT reasoning is displayed **separately in real-time** but excluded from history.
- **ğŸ“‹ Optimized Context Buffer** â€“ Ensures **only** the final answers are retained in session history.
- **ğŸ Debugging & Logging Tools**:
  - **Debug Mode:** Shows **real-time API payloads, CoT extraction, and model responses** for troubleshooting.
  - **Structured Logging:** Saves **full user queries, CoT reasoning, and responses** in session logs.
- **ğŸ’¾ Session Log Export** â€“ Allows easy export of chat logs with CoT insights.
- **âš¡ OpenAI API** â€“ For inferencing **on-premises and hosted instances of DeepSeek-R1**.

---

## ğŸ–¥ Screenshots

### **CoT Reasoning in Action**
![CoT in Action](screenshot_DeepTalk_CoT_open.png)

### **Collapsed CoT with LLM Output**
![LLM Output](screenshot_DeepTalk_CoT_closed.png)

---

## ğŸ“¦ Installation

Install the required dependencies:

```sh
pip install streamlit requests
```

---

## ğŸƒ Usage

Run the Streamlit DeepTalk interface:

```sh
streamlit run deeptalk.py
```

Specify your OpenAI-compatible API endpoint in the UI.

---

## ğŸ” How CoT Handling Works

1. **User submits a prompt.**  
2. **Model generates a response with CoT reasoning.**  
3. **The GUI captures and streams CoT reasoning separately.**  
4. **Once completed, CoT is removed from context memory.**  
5. **Only the final response is stored in chat history.**  

ğŸ’¡ *This method ensures that CoT reasoning improves responses without polluting future prompts.*

---

## ğŸ“œ License

Licensed under the **Apache 2.0 License**.  
See [LICENSE](LICENSE) for details.

---

## ğŸ¤ Contributing

Contributions are welcome!  
Fork the repo, make changes, and submit a pull request.

---

## ğŸ”— More Information

For further details and contributions, visit:  
[ğŸ”— GitHub Repository](https://github.com/AightBits/DeepTalk)
